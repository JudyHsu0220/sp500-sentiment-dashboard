# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hqWpib6xNj1SBW4TxPwkwclsN7r8-T8M
"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import altair as alt
from wordcloud import WordCloud, STOPWORDS
import ast
from datetime import datetime
import re
from collections import Counter
import string

# Load data
@st.cache_data

def load_data():
    df = pd.read_csv("merged_sentiment_cleaned_202005_202504.csv")
    df['date'] = pd.to_datetime(df['date'])
    df['nlp_features'] = df['nlp_features'].apply(ast.literal_eval)
    df['tokens'] = df['nlp_features'].apply(lambda x: x.get('tokens', []))
    df['month'] = df['date'].dt.to_period('M')
    return df

df = load_data()

# Sidebar filters
st.sidebar.title("Filters")
filter_mode = st.sidebar.radio("Filter by", ["Date Range", "Single Day"])

if filter_mode == "Date Range":
    start_date = st.sidebar.date_input("Start Date", df['date'].min())
    end_date = st.sidebar.date_input("End Date", df['date'].max())
    mask = (df['date'] >= pd.to_datetime(start_date)) & (df['date'] <= pd.to_datetime(end_date))
else:
    selected_date = st.sidebar.date_input("Select Date", value=pd.to_datetime("2024-12-01"))
    mask = df['date'] == pd.to_datetime(selected_date)

filtered_df = df[mask]

# Tabs
st.title("SP500 News Sentiment Dashboard")
tabs = st.tabs(["Sentiment vs Price", "Mention & Alert", "Word Cloud"])

# 1. SPX500 Sentiment Trendline
with tabs[0]:
    st.header("Sentiment and S&P500 Price Trend")

    price_df = pd.read_csv("sp500_price_202005_202504.csv")
    price_df['date'] = pd.to_datetime(price_df['date'])
    price_df = price_df[price_df['date'].isin(filtered_df['date'].unique())]

    daily_sentiment = filtered_df[filtered_df['related'] == 'S&P 500'].groupby('date')['sentiment'].mean()
    price_series = price_df.set_index('date')['close']

    aligned_dates = daily_sentiment.index.intersection(price_series.index)
    df_plot = pd.DataFrame({
        'date': aligned_dates,
        'Sentiment': daily_sentiment[aligned_dates].values,
        'Close Price': price_series[aligned_dates].values
    }).dropna()

    sentiment_range = [df_plot['Sentiment'].min(), df_plot['Sentiment'].max()]
    price_range = [df_plot['Close Price'].min(), df_plot['Close Price'].max()]

    base = alt.Chart(df_plot).encode(x='date:T')

    line_price = base.mark_line(color='blue').encode(
        y=alt.Y('Close Price:Q', scale=alt.Scale(domain=price_range), axis=alt.Axis(title='Price', titleColor='blue'))
    )

    line_sentiment = base.mark_line(color='orange').encode(
        y=alt.Y('Sentiment:Q', scale=alt.Scale(domain=sentiment_range), axis=alt.Axis(title='Sentiment', titleColor='orange'))
    )

    st.altair_chart(alt.layer(line_price, line_sentiment).resolve_scale(y='independent').interactive(), use_container_width=True)

    # Columns for stats
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Price Stats")
        st.metric("Min Price", round(price_series.min(), 2))
        st.metric("Max Price", round(price_series.max(), 2))
        st.metric("Mean Price", round(price_series.mean(), 2))
        st.metric("Std Dev Price", round(price_series.std(), 2))

    with col2:
        st.subheader("Sentiment Stats")
        st.metric("Min Sentiment", round(daily_sentiment.min(), 3))
        st.metric("Max Sentiment", round(daily_sentiment.max(), 3))
        st.metric("Mean Sentiment", round(daily_sentiment.mean(), 3))
        st.metric("Std Dev Sentiment", round(daily_sentiment.std(), 3))

# 2. Company Sentiment Summary
with tabs[1]:
    st.header("Company Mentions and Alerts")
    mention_df = filtered_df[filtered_df['related'] != 'S&P 500']
    summary = mention_df.groupby("related").agg(
        mention_count=('title', 'count'),
        avg_sentiment=('sentiment', 'mean')
    ).reset_index()
    summary['alert'] = summary['avg_sentiment'].apply(lambda x: '❗️' if x < -0.5 else '')

    st.dataframe(summary.sort_values("mention_count", ascending=False))

# 3. Keyword Cloud
with tabs[2]:
    st.header("Sentiment Word Cloud")

    # Join all tokens into lowercase and remove punctuation
    all_tokens_raw = [token.lower() for tokens in filtered_df['tokens'] for token in tokens if isinstance(token, str)]
    cleaned_tokens = [
        re.sub(r'[^\w\s]', '', token) for token in all_tokens_raw
        if token.isalpha()
    ]

    stopwords = set(STOPWORDS).union({
        'the', 'in', 'it', 'of', 'to', 'and', 'as', 'for', 'on', 'is', 'its', 'with', 'are', 'a', 'an', 'this', 'that'
    })

    filtered_tokens = [word for word in cleaned_tokens if word not in stopwords and len(word) > 1]

    wordcloud = WordCloud(
        width=1000, height=500, background_color='white', stopwords=stopwords
    ).generate(" ".join(filtered_tokens))

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    st.pyplot(fig)

    # Top 5 keywords and related headlines
    st.subheader("Top 5 Keywords and Related Headlines")
    top_tokens = Counter(filtered_tokens).most_common(5)

    for word, _ in top_tokens:
        st.markdown(f"**{word}**")
        try:
            pattern = re.compile(rf'\b{re.escape(word)}\b', flags=re.IGNORECASE)
            headlines = filtered_df[filtered_df['title'].str.contains(pattern, na=False)]['title'].head(5).tolist()
            for h in headlines:
                st.markdown(f"- {h}")
        except re.error:
            st.markdown("_Error parsing keyword pattern_")